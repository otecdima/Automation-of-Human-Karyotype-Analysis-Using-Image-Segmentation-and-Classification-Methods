{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automation of human karyotype analysis using image segmentation and classification methods. Segmentation"
      ],
      "metadata": {
        "id": "ERUyXkHDlG4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ezFt8VJ1aGfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1fUWGsTT9GMmQXt9NGqIcmLgaRyWMbWzg"
      ],
      "metadata": {
        "id": "ovBrmIl0tVg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Data.zip"
      ],
      "metadata": {
        "id": "Tk8Pbsqvum9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "k9aaqPNUvCVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN and Retinanet"
      ],
      "metadata": {
        "id": "wwkJGG7HaPow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
        "from functools import partial\n",
        "from torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\n",
        "import matplotlib.patches as patches\n",
        "import math\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import glob\n",
        "import random\n",
        "import shutil"
      ],
      "metadata": {
        "id": "Zx6ocS0HIfm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromosomeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, image_folder=\"images\", annotation_folder=\"annotations\", transforms=None):\n",
        "        self.root = root\n",
        "        self.image_folder = image_folder\n",
        "        self.annotation_folder = annotation_folder\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, image_folder))))\n",
        "        self.annotations = list(sorted(os.listdir(os.path.join(root, annotation_folder))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, self.image_folder, self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        annot_path = os.path.join(self.root, self.annotation_folder, self.annotations[idx])\n",
        "        tree = ET.parse(annot_path)\n",
        "        root_xml = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for obj in root_xml.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text.strip().lower()\n",
        "            labels.append(1)\n",
        "\n",
        "            bndbox = obj.find(\"bndbox\")\n",
        "            xmin = float(bndbox.find(\"xmin\").text)\n",
        "            ymin = float(bndbox.find(\"ymin\").text)\n",
        "            xmax = float(bndbox.find(\"xmax\").text)\n",
        "            ymax = float(bndbox.find(\"ymax\").text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = torch.tensor([idx])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "lo1xmDD-G1H6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_faster_rcnn(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def get_model_retinanet(num_classes):\n",
        "    # model = retinanet_resnet50_fpn(weights=None, weights_backbone=None)\n",
        "\n",
        "    # # Get the number of input channels from the final classification layer\n",
        "    # in_channels = model.head.classification_head.cls_logits.in_channels\n",
        "    # # Number of anchors (default is 9 for each feature map location)\n",
        "    # num_anchors = model.head.classification_head.num_anchors\n",
        "\n",
        "    # # Replace the classification head with a new one (for our custom num_classes)\n",
        "    # model.head.classification_head = RetinaNetClassificationHead(\n",
        "    #     in_channels=in_channels,\n",
        "    #     num_anchors=num_anchors,\n",
        "    #     num_classes=num_classes\n",
        "    # )\n",
        "\n",
        "    model = torchvision.models.detection.retinanet_resnet50_fpn_v2(\n",
        "        weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    )\n",
        "    num_anchors = model.head.classification_head.num_anchors\n",
        "    model.head.classification_head = RetinaNetClassificationHead(\n",
        "        in_channels=256,\n",
        "        num_anchors=num_anchors,\n",
        "        num_classes=num_classes,\n",
        "        norm_layer=partial(torch.nn.GroupNorm, 32)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "bgY24YXaHbpD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "root_dir = \"/content/Data/single_chromosomes_object\"\n",
        "image_folder = \"JEPG\"\n",
        "annotation_folder = \"anntations\"\n",
        "\n",
        "dataset = ChromosomeDataset(root=root_dir, image_folder=image_folder, annotation_folder=annotation_folder, transforms=get_transform(train=True))\n",
        "dataset_test = ChromosomeDataset(root=root_dir, image_folder=image_folder, annotation_folder=annotation_folder, transforms=get_transform(train=False))\n",
        "\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "split_index = int(0.8 * len(indices))\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:split_index])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[split_index:])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=lambda x: tuple(zip(*x))\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=lambda x: tuple(zip(*x))\n",
        ")"
      ],
      "metadata": {
        "id": "Gx9_fK7ReHdZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model = get_model_faster_rcnn(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch)\n",
        "\n",
        "torch.save(model.state_dict(), \"fasterrcnn_chromosomes.pth\")"
      ],
      "metadata": {
        "id": "v1LYm82yIASa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model = get_model_retinanet(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch)\n",
        "\n",
        "torch.save(model.state_dict(), \"retinanet_chromosomes.pth\")"
      ],
      "metadata": {
        "id": "o5_ju3wgINgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
        "\n",
        "def evaluate_image(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
        "\n",
        "    pred_boxes = sorted(pred_boxes, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    matched_gt = set()\n",
        "    tp = 0\n",
        "    for pred in pred_boxes:\n",
        "        best_iou = 0\n",
        "        best_idx = -1\n",
        "        for i, gt_box in enumerate(gt_boxes):\n",
        "            curr_iou = iou(pred['box'], gt_box)\n",
        "            if curr_iou > best_iou:\n",
        "                best_iou = curr_iou\n",
        "                best_idx = i\n",
        "\n",
        "        if best_iou >= iou_threshold and best_idx not in matched_gt:\n",
        "            tp += 1\n",
        "            matched_gt.add(best_idx)\n",
        "\n",
        "    fp = len(pred_boxes) - tp\n",
        "    fn = len(gt_boxes) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def compute_ap_single_class(all_preds, all_gts, iou_threshold=0.5):\n",
        "    pred_list = []\n",
        "    for img_id, preds in all_preds.items():\n",
        "        for p in preds:\n",
        "            pred_list.append({'image_id': img_id, 'box': p['box'], 'score': p['score']})\n",
        "    pred_list = sorted(pred_list, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    total_gts = sum(len(gts) for gts in all_gts.values())\n",
        "\n",
        "    tp_list = []\n",
        "    fp_list = []\n",
        "\n",
        "    matched = {img_id: np.zeros(len(all_gts[img_id])) for img_id in all_gts}\n",
        "\n",
        "    for pred in pred_list:\n",
        "        img_id = pred['image_id']\n",
        "        best_iou = 0\n",
        "        best_idx = -1\n",
        "        for i, gt_box in enumerate(all_gts[img_id]):\n",
        "            curr_iou = iou(pred['box'], gt_box)\n",
        "            if curr_iou > best_iou:\n",
        "                best_iou = curr_iou\n",
        "                best_idx = i\n",
        "\n",
        "        if best_iou >= iou_threshold:\n",
        "            if matched[img_id][best_idx] == 0:\n",
        "                tp_list.append(1)\n",
        "                fp_list.append(0)\n",
        "                matched[img_id][best_idx] = 1\n",
        "            else:\n",
        "                tp_list.append(0)\n",
        "                fp_list.append(1)\n",
        "        else:\n",
        "            tp_list.append(0)\n",
        "            fp_list.append(1)\n",
        "\n",
        "    tp_array = np.cumsum(tp_list)\n",
        "    fp_array = np.cumsum(fp_list)\n",
        "    precisions = tp_array / (tp_array + fp_array + 1e-6)\n",
        "    recalls = tp_array / (total_gts + 1e-6)\n",
        "\n",
        "    ap = 0.0\n",
        "    for t in np.linspace(0, 1, 11):\n",
        "        p = np.max(precisions[recalls >= t]) if np.any(recalls >= t) else 0\n",
        "        ap += p / 11.0\n",
        "\n",
        "    return ap\n",
        "\n",
        "\n",
        "def evaluate_detection_single_class(model, data_loader, device,\n",
        "                                    iou_threshold=0.5, score_threshold=0.5):\n",
        "    model.eval()\n",
        "    all_ground_truths = {}\n",
        "    all_predictions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, targets) in enumerate(data_loader):\n",
        "            image = images[0].to(device)\n",
        "            gt_boxes = targets[0]['boxes'].cpu().tolist()\n",
        "\n",
        "            outputs = model([image])\n",
        "            pred_boxes = []\n",
        "            for box, score in zip(outputs[0]['boxes'].cpu().tolist(),\n",
        "                                  outputs[0]['scores'].cpu().tolist()):\n",
        "                if score >= score_threshold:\n",
        "                    pred_boxes.append({'box': box, 'score': score})\n",
        "\n",
        "            all_ground_truths[idx] = gt_boxes\n",
        "            all_predictions[idx] = pred_boxes\n",
        "\n",
        "    metrics_per_image = {}\n",
        "    for img_id in all_ground_truths.keys():\n",
        "        prec, rec, f1 = evaluate_image(all_ground_truths[img_id],\n",
        "                                       all_predictions.get(img_id, []),\n",
        "                                       iou_threshold=iou_threshold)\n",
        "        metrics_per_image[img_id] = {\n",
        "            'precision': prec,\n",
        "            'recall': rec,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    ap = compute_ap_single_class(all_predictions, all_ground_truths, iou_threshold=iou_threshold)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(metrics_per_image, orient='index')\n",
        "    print(\"\\nPer-Image Metrics (IoU >= {:.2f}):\".format(iou_threshold))\n",
        "    print(df)\n",
        "    print(f\"\\nOverall AP@IoU={iou_threshold:.2f}: {ap:.3f}\")"
      ],
      "metadata": {
        "id": "zNltPuBuReN4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model_faster_rcnn = get_model_faster_rcnn(num_classes)\n",
        "model_faster_rcnn.load_state_dict(torch.load(\"fasterrcnn_chromosomes.pth\", map_location=device))\n",
        "model_faster_rcnn.to(device)\n",
        "\n",
        "evaluate_detection_single_class(\n",
        "        model=model_faster_rcnn,\n",
        "        data_loader=data_loader_test,\n",
        "        device=device,\n",
        "        iou_threshold=0.5,\n",
        "        score_threshold=0.5)"
      ],
      "metadata": {
        "id": "JIXStBynf2zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model_retinanet = get_model_retinanet(num_classes)\n",
        "model_retinanet.load_state_dict(torch.load(\"retinanet_chromosomes.pth\", map_location=device))\n",
        "model_retinanet.to(device)\n",
        "\n",
        "evaluate_detection_single_class(\n",
        "        model=model_retinanet,\n",
        "        data_loader=data_loader_test,\n",
        "        device=device,\n",
        "        iou_threshold=0.5,\n",
        "        score_threshold=0.5)"
      ],
      "metadata": {
        "id": "XfH3DmOnIswM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_coco_api(dataset):\n",
        "    coco_ds = {\n",
        "        \"images\": [],\n",
        "        \"categories\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "    coco_ds[\"categories\"].append({\n",
        "        \"id\": 1,\n",
        "        \"name\": \"chromosomes\"\n",
        "    })\n",
        "\n",
        "    annotation_id = 1\n",
        "    for img_idx in range(len(dataset)):\n",
        "        _, target = dataset[img_idx]\n",
        "\n",
        "        image_info = {\n",
        "            \"id\": img_idx,\n",
        "            \"file_name\": str(img_idx)\n",
        "        }\n",
        "        coco_ds[\"images\"].append(image_info)\n",
        "\n",
        "        boxes = target[\"boxes\"]\n",
        "        labels = target[\"labels\"]\n",
        "\n",
        "        boxes = boxes.numpy()\n",
        "        labels = labels.numpy()\n",
        "\n",
        "        for box, label in zip(boxes, labels):\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            w = xmax - xmin\n",
        "            h = ymax - ymin\n",
        "\n",
        "            ann = {\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": img_idx,\n",
        "                \"category_id\": int(label),\n",
        "                \"bbox\": [float(xmin), float(ymin), float(w), float(h)],\n",
        "                \"area\": float(w * h),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            coco_ds[\"annotations\"].append(ann)\n",
        "            annotation_id += 1\n",
        "\n",
        "    coco = COCO()\n",
        "    coco.dataset = coco_ds\n",
        "    coco.createIndex()\n",
        "    return coco\n",
        "\n",
        "def prepare_predictions(predictions, img_ids, label_offset=0):\n",
        "    coco_results = []\n",
        "    for img_id, prediction in zip(img_ids, predictions):\n",
        "        boxes = prediction[\"boxes\"].cpu().numpy()\n",
        "        scores = prediction[\"scores\"].cpu().numpy()\n",
        "        labels = prediction[\"labels\"].cpu().numpy()\n",
        "\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            w = xmax - xmin\n",
        "            h = ymax - ymin\n",
        "            coco_results.append({\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": int(label + label_offset),\n",
        "                \"bbox\": [float(xmin), float(ymin), float(w), float(h)],\n",
        "                \"score\": float(score)\n",
        "            })\n",
        "    return coco_results\n",
        "\n",
        "def coco_evaluate(model, data_loader, device, label_offset=0):\n",
        "    dataset = data_loader.dataset\n",
        "    if isinstance(dataset, torch.utils.data.Subset):\n",
        "        subset = dataset\n",
        "        dataset = subset.dataset\n",
        "        subset_indices = subset.indices\n",
        "    else:\n",
        "        subset_indices = range(len(dataset))\n",
        "\n",
        "    coco_gt = convert_to_coco_api(dataset)\n",
        "\n",
        "    model.eval()\n",
        "    results = []\n",
        "    img_ids = []\n",
        "\n",
        "    for i, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        batch_indices = subset_indices[i * data_loader.batch_size : i * data_loader.batch_size + len(images)]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        results.extend(prepare_predictions(outputs, batch_indices, label_offset=label_offset))\n",
        "        img_ids.extend(batch_indices)\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(results) if results else COCO()\n",
        "\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.params.imgIds = list(img_ids)\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval"
      ],
      "metadata": {
        "id": "GpfgW3eZYjr0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model_faster_rcnn = get_model_faster_rcnn(num_classes)\n",
        "model_faster_rcnn.load_state_dict(torch.load(\"fasterrcnn_chromosomes.pth\", map_location=device))\n",
        "model_faster_rcnn.to(device)\n",
        "\n",
        "coco_eval_faster_rcnn = coco_evaluate(model_faster_rcnn, data_loader_test, device, label_offset=0)"
      ],
      "metadata": {
        "id": "tykWHr2qbq2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "model_retinanet = get_model_retinanet(num_classes)\n",
        "model_retinanet.load_state_dict(torch.load(\"retinanet_chromosomes.pth\", map_location=device))\n",
        "model_retinanet.to(device)\n",
        "\n",
        "coco_eval_retinanet = coco_evaluate(model_retinanet, data_loader_test, device, label_offset=0)"
      ],
      "metadata": {
        "id": "2rW69LB-btqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_sample(model, dataset, idx=0, threshold=0.5, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    model.eval()\n",
        "\n",
        "    img, target = dataset[idx]\n",
        "    image_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 7))\n",
        "\n",
        "    axs[0].imshow(image_np)\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    axs[1].imshow(image_np)\n",
        "    axs[1].set_title(\"Ground Truth Boxes\")\n",
        "\n",
        "    for box in target[\"boxes\"]:\n",
        "        xmin, ymin, xmax, ymax = box.tolist()\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor=\"green\", facecolor=\"none\")\n",
        "        axs[1].add_patch(rect)\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    img_tensor = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img_tensor])[0]\n",
        "\n",
        "    axs[2].imshow(image_np)\n",
        "    axs[2].set_title(\"Predicted Boxes\")\n",
        "\n",
        "    for box, score in zip(prediction[\"boxes\"], prediction[\"scores\"]):\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        axs[2].add_patch(rect)\n",
        "    axs[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_sample(model_faster_rcnn, dataset_test, idx=0, threshold=0.5)\n",
        "visualize_sample(model_retinanet, dataset_test, idx=0, threshold=0.5)"
      ],
      "metadata": {
        "id": "31x3finrZ0w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO"
      ],
      "metadata": {
        "id": "uIAkTrd0NJ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_voc_to_yolo(xml_file, labels_dir, image_dir):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    filename = root.find('filename').text\n",
        "    image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "    size_tag = root.find('size')\n",
        "    img_width = int(size_tag.find('width').text)\n",
        "    img_height = int(size_tag.find('height').text)\n",
        "\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "    txt_file = os.path.join(labels_dir, base_name + '.txt')\n",
        "\n",
        "    lines = []\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        class_id = 0\n",
        "\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = float(bndbox.find('xmin').text)\n",
        "        ymin = float(bndbox.find('ymin').text)\n",
        "        xmax = float(bndbox.find('xmax').text)\n",
        "        ymax = float(bndbox.find('ymax').text)\n",
        "\n",
        "        x_center = ((xmin + xmax) / 2.0) / img_width\n",
        "        y_center = ((ymin + ymax) / 2.0) / img_height\n",
        "        w = (xmax - xmin) / img_width\n",
        "        h = (ymax - ymin) / img_height\n",
        "\n",
        "        line = f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\"\n",
        "        lines.append(line)\n",
        "\n",
        "    with open(txt_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "\n",
        "def prepare_dataset(data_dir, train_split=0.8):\n",
        "    images_dir = os.path.join(data_dir, \"JEPG\")\n",
        "    annotations_dir = os.path.join(data_dir, \"anntations\")\n",
        "\n",
        "    output_images_train = os.path.join(data_dir, \"images\", \"train\")\n",
        "    output_images_val = os.path.join(data_dir, \"images\", \"val\")\n",
        "    output_labels_train = os.path.join(data_dir, \"labels\", \"train\")\n",
        "    output_labels_val = os.path.join(data_dir, \"labels\", \"val\")\n",
        "\n",
        "    os.makedirs(output_images_train, exist_ok=True)\n",
        "    os.makedirs(output_images_val, exist_ok=True)\n",
        "    os.makedirs(output_labels_train, exist_ok=True)\n",
        "    os.makedirs(output_labels_val, exist_ok=True)\n",
        "\n",
        "    xml_files = glob.glob(os.path.join(annotations_dir, \"*.xml\"))\n",
        "\n",
        "    random.shuffle(xml_files)\n",
        "    train_count = int(len(xml_files) * train_split)\n",
        "    train_xmls = xml_files[:train_count]\n",
        "    val_xmls   = xml_files[train_count:]\n",
        "\n",
        "    def move_and_convert(xml_list, images_subdir, labels_subdir):\n",
        "        for xml_file in xml_list:\n",
        "            convert_voc_to_yolo(xml_file, labels_subdir, images_dir)\n",
        "\n",
        "            tree = ET.parse(xml_file)\n",
        "            root = tree.getroot()\n",
        "            filename = root.find('filename').text\n",
        "            src_img_path = os.path.join(images_dir, filename)\n",
        "            dst_img_path = os.path.join(images_subdir, filename)\n",
        "            if os.path.exists(src_img_path):\n",
        "                shutil.copy2(src_img_path, dst_img_path)\n",
        "\n",
        "    move_and_convert(train_xmls, output_images_train, output_labels_train)\n",
        "    move_and_convert(val_xmls,   output_images_val,   output_labels_val)\n",
        "\n",
        "    data_yaml = os.path.join(data_dir, \"data.yaml\")\n",
        "    with open(data_yaml, 'w') as f:\n",
        "        f.write(\"train: {}/images/train\\n\".format(data_dir))\n",
        "        f.write(\"val: {}/images/val\\n\".format(data_dir))\n",
        "        f.write(\"names: ['chromosomes']\\n\")\n",
        "\n",
        "\n",
        "def train_yolo(data_dir, model_size='n', epochs=50, imgsz=640):\n",
        "    data_yaml = os.path.join(data_dir, \"data.yaml\")\n",
        "    model_name = f\"yolov8{model_size}.pt\" #'n', 's', 'm', 'l', 'x'\n",
        "    model = YOLO(model_name)\n",
        "\n",
        "    model.train(\n",
        "        data=data_yaml,\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        project=os.path.join(data_dir, \"runs\"),\n",
        "        name=f\"yolo_chromosomes_{model_size}\",\n",
        "        exist_ok=True\n",
        "    )\n",
        "\n",
        "DATA_DIR = \"/content/Data/single_chromosomes_object\"\n",
        "prepare_dataset(DATA_DIR, train_split=0.8)\n",
        "\n",
        "train_yolo(\n",
        "    data_dir=DATA_DIR,\n",
        "    model_size='n',\n",
        "    epochs=50,\n",
        "    imgsz=640\n",
        ")"
      ],
      "metadata": {
        "id": "qXftW4PuHOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(data_dir, model_path, test_image_path):\n",
        "    model = YOLO(model_path)\n",
        "    results = model.predict(source=test_image_path, conf=0.25, save=True)\n",
        "\n",
        "best_model_path = os.path.join(DATA_DIR, \"runs\", \"yolo_chromosomes_n\", \"weights\", \"best.pt\")\n",
        "\n",
        "test_image = os.path.join(DATA_DIR, \"images\", \"train\", \"103064.jpg\")\n",
        "\n",
        "run_inference(\n",
        "    data_dir=DATA_DIR,\n",
        "    model_path=best_model_path,\n",
        "    test_image_path=test_image\n",
        ")"
      ],
      "metadata": {
        "id": "_4Ts2QuXIFQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_voc_annotations(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "    for obj in root.findall('object'):\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = int(float(bndbox.find('xmin').text))\n",
        "        ymin = int(float(bndbox.find('ymin').text))\n",
        "        xmax = int(float(bndbox.find('xmax').text))\n",
        "        ymax = int(float(bndbox.find('ymax').text))\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def draw_boxes_cv2(image, boxes, color=(0, 255, 0), label_text=None):\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness=2)\n",
        "        if label_text:\n",
        "            cv2.putText(image, label_text, (x1, max(0, y1 - 5)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def plot_original_gt_pred(image_path, xml_path, model_path, conf_threshold=0.25):\n",
        "    original_bgr = cv2.imread(image_path)\n",
        "    if original_bgr is None:\n",
        "        raise FileNotFoundError(f\"Could not read image at: {image_path}\")\n",
        "\n",
        "    gt_bgr = original_bgr.copy()\n",
        "    pred_bgr = original_bgr.copy()\n",
        "\n",
        "    if xml_path and os.path.exists(xml_path):\n",
        "        gt_boxes = parse_voc_annotations(xml_path)\n",
        "        gt_bgr = draw_boxes_cv2(gt_bgr, gt_boxes, color=(0, 255, 0))\n",
        "    else:\n",
        "        print(\"No valid XML file found\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "    results = model.predict(source=image_path, conf=conf_threshold)\n",
        "\n",
        "    pred_boxes = []\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0]\n",
        "            pred_boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
        "\n",
        "    pred_bgr = draw_boxes_cv2(pred_bgr, pred_boxes, color=(0, 0, 255))\n",
        "\n",
        "    original_rgb = cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)\n",
        "    gt_rgb = cv2.cvtColor(gt_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pred_rgb = cv2.cvtColor(pred_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
        "\n",
        "    axs[0].imshow(original_rgb)\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    axs[1].imshow(gt_rgb)\n",
        "    axs[1].set_title(\"Ground Truth Boxes\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    axs[2].imshow(pred_rgb)\n",
        "    axs[2].set_title(\"Predicted Boxes\")\n",
        "    axs[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "best_model_path = \"/content/Data/single_chromosomes_object/runs/yolo_chromosomes_n/weights/best.pt\"\n",
        "test_image_path = \"/content/Data/single_chromosomes_object/images/train/103064.jpg\"\n",
        "xml_path = \"/content/Data/single_chromosomes_object/anntations/103064.xml\"\n",
        "\n",
        "plot_original_gt_pred(\n",
        "    image_path=test_image_path,\n",
        "    xml_path=xml_path,\n",
        "    model_path=best_model_path,\n",
        "    conf_threshold=0.25\n",
        ")"
      ],
      "metadata": {
        "id": "4xHXSE7DiO7E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}